{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9738139,"sourceType":"datasetVersion","datasetId":5960377}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision efficientnet_pytorch pandas scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:16:38.143582Z","iopub.execute_input":"2024-10-27T20:16:38.143968Z","iopub.status.idle":"2024-10-27T20:16:53.080010Z","shell.execute_reply.started":"2024-10-27T20:16:38.143931Z","shell.execute_reply":"2024-10-27T20:16:53.079040Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nCollecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=5549c08e8f240f1d88f5ce38579639a18153a6acf44fbe2158f79dde7623cb64\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom torch import nn, optim\nfrom tqdm import tqdm\nfrom PIL import Image\nimport pandas as pd\nimport torch\nimport copy\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:28.347198Z","iopub.execute_input":"2024-10-27T20:21:28.347968Z","iopub.status.idle":"2024-10-27T20:21:28.353802Z","shell.execute_reply.started":"2024-10-27T20:21:28.347926Z","shell.execute_reply":"2024-10-27T20:21:28.352818Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Set device (use GPU ONLY)\n\ndevice = torch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:29.796283Z","iopub.execute_input":"2024-10-27T20:21:29.796682Z","iopub.status.idle":"2024-10-27T20:21:29.801251Z","shell.execute_reply.started":"2024-10-27T20:21:29.796641Z","shell.execute_reply":"2024-10-27T20:21:29.800259Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class DiabeticRetinopathyDataset(Dataset):\n    def __init__(self, csv_file, image_dir, transform=None, is_test=False):\n        self.data = pd.read_csv(csv_file)\n        self.image_dir = image_dir  # Image directory\n        self.transform = transform\n        self.is_test = is_test\n        self.skip_images = {'194_right.jpeg', '29126_right.jpeg', '38790_right.jpeg', '8421_left.jpeg'}  # Images to skip\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_name = self.data.iloc[idx]['image']\n\n        # Skip specified images and return a dummy tensor and a default label\n        if image_name in self.skip_images:\n            print(f\"Skipping image {image_name} as per request\")\n            dummy_image = torch.zeros(3, 224, 224)  # A dummy image tensor (3x224x224)\n            return dummy_image, 0  # Returning dummy tensor and label '0'\n\n        image_path = os.path.join(self.image_dir, image_name)  # Path to the image\n        try:\n            image = Image.open(image_path).convert('RGB')\n        except (OSError, IOError) as e:\n            print(f\"Error loading image {image_name}, skipping. Error: {e}\")\n            dummy_image = torch.zeros(3, 224, 224)\n            return dummy_image, 0\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.is_test:\n            return image, image_name\n        else:\n            label = self.data.iloc[idx]['level']\n            return image, label","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:30.117529Z","iopub.execute_input":"2024-10-27T20:21:30.118459Z","iopub.status.idle":"2024-10-27T20:21:30.128510Z","shell.execute_reply.started":"2024-10-27T20:21:30.118417Z","shell.execute_reply":"2024-10-27T20:21:30.127490Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Define image transformations\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:30.864296Z","iopub.execute_input":"2024-10-27T20:21:30.864685Z","iopub.status.idle":"2024-10-27T20:21:30.871647Z","shell.execute_reply.started":"2024-10-27T20:21:30.864646Z","shell.execute_reply":"2024-10-27T20:21:30.870585Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Load data\n\ntrain_df = pd.read_csv('/kaggle/input/diabetic-retinopathy/train.csv')\ntrain_data, val_data = train_test_split(train_df, test_size=0.4, random_state=42)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:31.139435Z","iopub.execute_input":"2024-10-27T20:21:31.140275Z","iopub.status.idle":"2024-10-27T20:21:31.187272Z","shell.execute_reply.started":"2024-10-27T20:21:31.140234Z","shell.execute_reply":"2024-10-27T20:21:31.186328Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                  image  level\n0          10_left.jpeg      0\n1         10_right.jpeg      0\n2          13_left.jpeg      0\n3         13_right.jpeg      0\n4          15_left.jpeg      1\n...                 ...    ...\n33694  44347_right.jpeg      0\n33695   44348_left.jpeg      0\n33696  44348_right.jpeg      0\n33697   44349_left.jpeg      0\n33698  44349_right.jpeg      1\n\n[33699 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15_left.jpeg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33694</th>\n      <td>44347_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33695</th>\n      <td>44348_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33696</th>\n      <td>44348_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33697</th>\n      <td>44349_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33698</th>\n      <td>44349_right.jpeg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33699 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save train and val splits to temporary CSVs\n\ntrain_image_dir = \"/kaggle/input/diabetic-retinopathy/train/\"\ntest_image_dir = \"/kaggle/input/diabetic-retinopathy/test/\"\n\ntrain_data.to_csv('/kaggle/working/train_split.csv', index=False)\nval_data.to_csv('/kaggle/working/val_split.csv', index=False)\n\ntrain_dataset = DiabeticRetinopathyDataset('train_split.csv', image_dir=train_image_dir, transform=train_transform)\nval_dataset = DiabeticRetinopathyDataset('val_split.csv', image_dir=train_image_dir, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:31.410287Z","iopub.execute_input":"2024-10-27T20:21:31.410965Z","iopub.status.idle":"2024-10-27T20:21:31.505435Z","shell.execute_reply.started":"2024-10-27T20:21:31.410922Z","shell.execute_reply":"2024-10-27T20:21:31.504468Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Initialize EfficientNet model\n\nmodel = EfficientNet.from_pretrained('efficientnet-b0')\nnum_ftrs = model._fc.in_features\nmodel._fc = nn.Linear(num_ftrs, 5)  # 5 output classes (0-4 for diabetic retinopathy)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:32.797656Z","iopub.execute_input":"2024-10-27T20:21:32.798083Z","iopub.status.idle":"2024-10-27T20:21:32.971701Z","shell.execute_reply.started":"2024-10-27T20:21:32.798035Z","shell.execute_reply":"2024-10-27T20:21:32.970579Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss function and optimizer\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR every 5 epochs","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:34.918492Z","iopub.execute_input":"2024-10-27T20:21:34.919344Z","iopub.status.idle":"2024-10-27T20:21:34.925964Z","shell.execute_reply.started":"2024-10-27T20:21:34.919304Z","shell.execute_reply":"2024-10-27T20:21:34.924930Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Function to evaluate the model on validation set\n\ndef evaluate(model, val_loader, criterion):\n    \n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        \n        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    \n    val_accuracy = correct / total\n    val_loss /= len(val_loader)\n    \n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:35.186724Z","iopub.execute_input":"2024-10-27T20:21:35.187106Z","iopub.status.idle":"2024-10-27T20:21:35.194834Z","shell.execute_reply.started":"2024-10-27T20:21:35.187070Z","shell.execute_reply":"2024-10-27T20:21:35.193787Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Function to train the model\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_accuracy = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in tqdm(train_loader, desc=\"Training\"):\n            # Skip if image or label is None (invalid/truncated image)\n            if images is None or labels is None:\n                continue\n\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_acc = correct / total\n        print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}')\n\n        # Validation phase\n        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n\n        # Save the best model\n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            best_model_wts = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), f'/kaggle/working/best_model_epoch_{epoch+1}.pth')\n            print(f'Saved best model for epoch {epoch+1} at /kaggle/working/')\n\n        # Step the learning rate scheduler\n        scheduler.step()\n\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:36.485821Z","iopub.execute_input":"2024-10-27T20:21:36.486644Z","iopub.status.idle":"2024-10-27T20:21:36.497005Z","shell.execute_reply.started":"2024-10-27T20:21:36.486581Z","shell.execute_reply":"2024-10-27T20:21:36.496069Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Train the model\n\nmodel = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:21:37.042476Z","iopub.execute_input":"2024-10-27T20:21:37.043186Z","iopub.status.idle":"2024-10-28T05:18:25.428867Z","shell.execute_reply.started":"2024-10-27T20:21:37.043143Z","shell.execute_reply":"2024-10-28T05:18:25.427694Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  17%|█▋        | 213/1264 [02:43<09:22,  1.87it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  36%|███▌      | 451/1264 [05:44<11:52,  1.14it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  84%|████████▍ | 1062/1264 [13:32<01:57,  1.71it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  89%|████████▉ | 1123/1264 [14:17<01:36,  1.46it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:03<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7533, Training Accuracy: 0.7486\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:47<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6912, Validation Accuracy: 0.7697\nSaved best model for epoch 1 at /kaggle/working/\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  27%|██▋       | 344/1264 [04:23<11:12,  1.37it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|███▎      | 422/1264 [05:23<07:14,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|█████▍    | 689/1264 [08:51<06:47,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  92%|█████████▏| 1165/1264 [14:58<01:05,  1.51it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:13<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6788, Training Accuracy: 0.7762\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:46<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6622, Validation Accuracy: 0.7849\nSaved best model for epoch 2 at /kaggle/working/\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  11%|█         | 142/1264 [01:51<14:45,  1.27it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  47%|████▋     | 598/1264 [07:39<05:22,  2.06it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  60%|█████▉    | 757/1264 [09:42<05:43,  1.48it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  89%|████████▉ | 1122/1264 [14:22<01:13,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:12<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6504, Training Accuracy: 0.7831\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:40<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.9842, Validation Accuracy: 0.5719\nEpoch 4/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|█▍        | 189/1264 [02:24<13:15,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  31%|███       | 389/1264 [04:56<07:56,  1.84it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  50%|█████     | 632/1264 [08:05<06:54,  1.53it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  69%|██████▉   | 874/1264 [11:09<04:24,  1.48it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:09<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6280, Training Accuracy: 0.7922\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:46<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6664, Validation Accuracy: 0.7863\nSaved best model for epoch 4 at /kaggle/working/\nEpoch 5/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 13/1264 [00:13<19:58,  1.04it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  42%|████▏     | 526/1264 [06:44<07:23,  1.66it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  52%|█████▏    | 661/1264 [08:29<05:31,  1.82it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|█████▍    | 685/1264 [08:48<05:31,  1.75it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:15<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6108, Training Accuracy: 0.7981\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:44<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7085, Validation Accuracy: 0.7552\nEpoch 6/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|▍         | 57/1264 [00:45<19:10,  1.05it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  67%|██████▋   | 846/1264 [10:43<03:29,  1.99it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  85%|████████▍ | 1074/1264 [13:36<01:34,  2.01it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  87%|████████▋ | 1101/1264 [13:58<01:42,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:04<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5500, Training Accuracy: 0.8177\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:30<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5779, Validation Accuracy: 0.8138\nSaved best model for epoch 6 at /kaggle/working/\nEpoch 7/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  40%|████      | 508/1264 [06:31<07:26,  1.69it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  64%|██████▎   | 805/1264 [10:19<04:06,  1.86it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\nSkipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  67%|██████▋   | 853/1264 [10:56<02:52,  2.39it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:13<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5260, Training Accuracy: 0.8256\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:30<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5699, Validation Accuracy: 0.8171\nSaved best model for epoch 7 at /kaggle/working/\nEpoch 8/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/1264 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  11%|█         | 134/1264 [01:44<16:39,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|█▌        | 190/1264 [02:28<14:26,  1.24it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  39%|███▉      | 491/1264 [06:17<08:10,  1.57it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:04<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5172, Training Accuracy: 0.8317\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:40<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5671, Validation Accuracy: 0.8165\nEpoch 9/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  18%|█▊        | 231/1264 [03:01<11:02,  1.56it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  65%|██████▍   | 821/1264 [10:33<08:32,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  99%|█████████▉| 1250/1264 [16:02<00:08,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\nSkipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:13<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.5038, Training Accuracy: 0.8334\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:29<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5707, Validation Accuracy: 0.8145\nEpoch 10/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  41%|████      | 514/1264 [06:32<08:18,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  83%|████████▎ | 1052/1264 [13:31<04:25,  1.25s/it]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  92%|█████████▏| 1163/1264 [14:55<00:59,  1.69it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  96%|█████████▌| 1210/1264 [15:30<00:34,  1.59it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:11<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4920, Training Accuracy: 0.8381\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:23<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5653, Validation Accuracy: 0.8157\nEpoch 11/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|▎         | 45/1264 [00:34<12:23,  1.64it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  18%|█▊        | 226/1264 [02:53<13:52,  1.25it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  28%|██▊       | 352/1264 [04:31<08:31,  1.78it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  78%|███████▊  | 992/1264 [12:46<02:34,  1.76it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:15<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4789, Training Accuracy: 0.8396\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:43<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5636, Validation Accuracy: 0.8168\nEpoch 12/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   7%|▋         | 86/1264 [01:08<12:33,  1.56it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|█▌        | 194/1264 [02:31<09:07,  1.95it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|█████▎    | 679/1264 [08:46<05:49,  1.67it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  58%|█████▊    | 738/1264 [09:32<05:50,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:18<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4735, Training Accuracy: 0.8434\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:24<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5642, Validation Accuracy: 0.8178\nSaved best model for epoch 12 at /kaggle/working/\nEpoch 13/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  26%|██▌       | 328/1264 [04:09<08:16,  1.88it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  35%|███▌      | 448/1264 [05:41<05:55,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  65%|██████▌   | 824/1264 [10:33<04:28,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  87%|████████▋ | 1096/1264 [14:03<02:10,  1.29it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:12<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4747, Training Accuracy: 0.8408\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:24<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5651, Validation Accuracy: 0.8168\nEpoch 14/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  44%|████▍     | 553/1264 [07:03<07:33,  1.57it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  64%|██████▍   | 809/1264 [10:20<04:08,  1.83it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  79%|███████▉  | 1000/1264 [12:48<02:14,  1.97it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  94%|█████████▍| 1185/1264 [15:10<00:40,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:11<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4758, Training Accuracy: 0.8424\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:44<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5655, Validation Accuracy: 0.8181\nSaved best model for epoch 14 at /kaggle/working/\nEpoch 15/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|▌         | 65/1264 [00:52<19:27,  1.03it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  35%|███▍      | 439/1264 [05:40<08:21,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  46%|████▌     | 579/1264 [07:29<07:14,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|█████▌    | 699/1264 [09:00<06:29,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:13<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4723, Training Accuracy: 0.8430\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:29<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5649, Validation Accuracy: 0.8175\nEpoch 16/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  61%|██████▏   | 775/1264 [09:59<04:26,  1.83it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  63%|██████▎   | 795/1264 [10:14<04:06,  1.91it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  79%|███████▉  | 1000/1264 [12:50<02:05,  2.11it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  98%|█████████▊| 1236/1264 [15:53<00:19,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:13<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4684, Training Accuracy: 0.8429\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:28<00:00,  1.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5658, Validation Accuracy: 0.8163\nEpoch 17/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   9%|▉         | 114/1264 [01:29<13:49,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  24%|██▎       | 298/1264 [03:49<09:18,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  84%|████████▍ | 1060/1264 [13:37<02:00,  1.69it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  98%|█████████▊| 1236/1264 [15:50<00:15,  1.85it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:11<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4711, Training Accuracy: 0.8435\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:45<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5655, Validation Accuracy: 0.8164\nEpoch 18/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  52%|█████▏    | 662/1264 [08:34<05:10,  1.94it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  70%|██████▉   | 883/1264 [11:25<03:40,  1.72it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  76%|███████▋  | 965/1264 [12:27<03:30,  1.42it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  81%|████████▏ | 1030/1264 [13:17<02:27,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:18<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4679, Training Accuracy: 0.8458\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:44<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5659, Validation Accuracy: 0.8175\nEpoch 19/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|▍         | 48/1264 [00:38<15:01,  1.35it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  29%|██▉       | 371/1264 [04:43<12:17,  1.21it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  45%|████▍     | 567/1264 [07:16<10:31,  1.10it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  65%|██████▍   | 821/1264 [10:36<05:20,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:25<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4675, Training Accuracy: 0.8430\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:32<00:00,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5653, Validation Accuracy: 0.8165\nEpoch 20/20\n","output_type":"stream"},{"name":"stderr","text":"Training:   1%|          | 12/1264 [00:10<12:06,  1.72it/s] ","output_type":"stream"},{"name":"stdout","text":"Skipping image 29126_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  70%|███████   | 889/1264 [11:34<03:37,  1.72it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 8421_left.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  84%|████████▍ | 1065/1264 [13:51<02:15,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 38790_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training:  85%|████████▍ | 1069/1264 [13:54<02:05,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"Skipping image 194_right.jpeg as per request\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1264/1264 [16:21<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.4695, Training Accuracy: 0.8420\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 843/843 [10:47<00:00,  1.30it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5658, Validation Accuracy: 0.8168\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test predictions\n\nmodel_path = \"/kaggle/working/best_model_epoch_14.pth\"\nmodel.load_state_dict(torch.load(model_path))  # Load the state_dict from the saved model\nmodel.to(device)\n\ntest_dataset = DiabeticRetinopathyDataset('/kaggle/input/diabetic-retinopathy/test.csv', image_dir=test_image_dir, transform=val_transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for images, image_paths in tqdm(test_loader, desc=\"Testing\"):\n        images = images.to(device)\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(list(zip(image_paths, preds.cpu().numpy())))","metadata":{"execution":{"iopub.status.busy":"2024-10-28T05:24:48.809066Z","iopub.execute_input":"2024-10-28T05:24:48.809468Z","iopub.status.idle":"2024-10-28T05:38:29.640844Z","shell.execute_reply.started":"2024-10-28T05:24:48.809425Z","shell.execute_reply":"2024-10-28T05:38:29.639698Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2401850270.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))  # Load the state_dict from the saved model\nTesting:  50%|████▉     | 523/1055 [06:47<06:58,  1.27it/s]","output_type":"stream"},{"name":"stdout","text":"Error loading image 1626_left.jpeg, skipping. Error: image file is truncated (38 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|█████████▉| 1051/1055 [13:39<00:03,  1.30it/s]","output_type":"stream"},{"name":"stdout","text":"Error loading image 22554_right.jpeg, skipping. Error: image file is truncated (47 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 1055/1055 [13:40<00:00,  1.29it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save predictions to CSV\n\npred_df = pd.DataFrame(predictions, columns=['image', 'level'])\npred_df.to_csv('/kaggle/working/predictions.csv', index=False)\nprint(\"saved to predictions.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T05:43:02.793395Z","iopub.execute_input":"2024-10-28T05:43:02.793919Z","iopub.status.idle":"2024-10-28T05:43:02.899575Z","shell.execute_reply.started":"2024-10-28T05:43:02.793877Z","shell.execute_reply":"2024-10-28T05:43:02.898585Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"saved to predictions.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"image1 = Image.open('/kaggle/input/diabetic-retinopathy/train/10007_right.jpeg').convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T20:11:59.458925Z","iopub.execute_input":"2024-10-27T20:11:59.459364Z","iopub.status.idle":"2024-10-27T20:11:59.567974Z","shell.execute_reply.started":"2024-10-27T20:11:59.459322Z","shell.execute_reply":"2024-10-27T20:11:59.567167Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"asfasdf = pd.read_csv('/kaggle/working/predictions.csv')\nasfasdf","metadata":{"execution":{"iopub.status.busy":"2024-10-28T05:43:34.780911Z","iopub.execute_input":"2024-10-28T05:43:34.781283Z","iopub.status.idle":"2024-10-28T05:43:34.817789Z","shell.execute_reply.started":"2024-10-28T05:43:34.781250Z","shell.execute_reply":"2024-10-28T05:43:34.816850Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                  image  level\n0       10000_left.jpeg      0\n1      10000_right.jpeg      0\n2       10001_left.jpeg      2\n3      10001_right.jpeg      2\n4       10002_left.jpeg      0\n...                 ...    ...\n16867  22552_right.jpeg      2\n16868   22553_left.jpeg      0\n16869  22553_right.jpeg      0\n16870   22554_left.jpeg      0\n16871                 0      0\n\n[16872 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>level</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10000_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10000_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10001_left.jpeg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10001_right.jpeg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10002_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16867</th>\n      <td>22552_right.jpeg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16868</th>\n      <td>22553_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16869</th>\n      <td>22553_right.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16870</th>\n      <td>22554_left.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16871</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16872 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}